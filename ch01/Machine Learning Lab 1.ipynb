{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Lab \\#1\n",
    "\n",
    "This notebook is on github at https://goo.gl/qfKVde. \n",
    "\n",
    "## Software Setup\n",
    "*When writing this section, I took 20 minutes. I expect you will take a bit less. *\n",
    "\n",
    "According to my own tests, it looks like you can install and run all of the software you need on on the lab machines without requiring root privileges.  In this section I have provided the comands I used in install python and the necessary libraries. You may copy and paste these into a terminal on your lab machine. To open a terminal I use `⌘+[SPACE]`   and type `terminal`. \n",
    "You can pastw each line into a terminal on the OSX lab machines\n",
    "\n",
    "```bash\n",
    "$ curl https://repo.continuum.io/archive/Anaconda2-4.4.0-MacOSX-x86_64.sh -O\n",
    "$ bash Anaconda2-4.4.0-MacOSX-x86_64.sh\n",
    "```\n",
    "This will start an installation program that will ask you questions. Accept license and all default settings when prompted.  When it finishes, it will add a line to your `.bash_profile` file so that anaconda (and the python related tools) are prepended to your `PATH` environment variable.   You may either  add [path/to/anaconda2] to PATH yourself, or just re-run the `.bash_profile` script (as I have done)\n",
    "\n",
    "```bash\n",
    "$ source ~/.bash_profile\n",
    "```\n",
    "\n",
    "At this point in time the `conda` program should be on your search path. You can test it by typing `which conda`, and/or `conda --version`. \n",
    "\n",
    "\n",
    "One of the most frustrating things about developing software is when “it works on my machine” (but only mine).  Related to this is the problem that happens when something works until you update your libraries for some other project, then you return to old code and it is broken. \n",
    "\n",
    "\n",
    "To solve this, python programmers use virtual environments.  All of the libraries you need are installed into a self-contained environment, within which you can make sure you have the packages your software needs.  To create an environment for this project use this command line:\n",
    "\n",
    "```bash\n",
    "$ conda create -n ser627a anaconda jupyter numpy scipy scikit-learn\n",
    "```\n",
    "\n",
    "Let’s break that command down:\n",
    "* `conda` is a program with a number of subcommands (just like `git` is). The second token after `conda` is the subcommand. \n",
    "* The `create` subcommand creates a new virtual environment that will also use the `conda` package manager.\n",
    "* The `-n ser627a`  options names the new environment. \n",
    "* The `anaconda`  package is actually a meta-package, it installs all of the default / useful python packages that python programmers tend to assume are present. \n",
    "* The `jupyter` package is the jupyter notebook we will be using as our python editor. This `.ipynb` file you are currently reading is the type of ducument that `jupyter` helps you to create. \n",
    "* The `numpy` package is the numerical N-Dimensional array (tensor) package.\n",
    "* The `scipy` package is another meta-package with various tools that are useful for scientific computing. \n",
    "* The `scikit-learn` package is a suite of tools that are useful for machine learning in python.\n",
    "\n",
    "Note that you will need to activate  the environment before these packages become available to you. \n",
    "\n",
    "```bash\n",
    "$ source activate ser627a\n",
    "```\n",
    "Since you want `conda` to change environment variables in __your__ process, you must use `source` or a period `.` to `.` to run the `activate` script without creating a child process. \n",
    "\n",
    "You should see the word `(ser627a)` prepended to your prompt in the terminal\n",
    "If you want to install new software packages while inside of an environment, you may use `conda install <packagename>` to install it. For example:\n",
    "\n",
    "```bash\n",
    "(ser627a)$ conda install configargparse\n",
    "```\n",
    "\n",
    "While in your `ser627a` environment, start a jupyter notebook web server by typing this:\n",
    "\n",
    "```bash\n",
    "(ser627a)$ jupyter notebook\n",
    "```\n",
    "\n",
    "Your browser should pop up, you can now use your jupyter notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Up until now, you may have been reading this in a browser. \n",
    ">Before continuing, you should download and open this notebook. \n",
    "\n",
    ">In a second terminal, change directories into the folder that you have `jupyter notebook` running.  From there >you can download this notebook:\n",
    "\n",
    "curl https://raw.githubusercontent.com/cse627A/chapter1/master/ch01/Machine%20Learning%20Lab%201.ipynb -o lab1.ipynb\n",
    "\n",
    "\n",
    "> Now look for the downloaded notebook in the browser that popped up when you ran jupyter, and begin editing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a `jupyter` notebook you edit code in cells, which are executed when you press `SHIFT+ENTER`. Similar to tools like `vim`, the `jupyter` editor allows you to run some extra editin commands by pressing the `ESC` key. You can also access most of those commands through the menus. The most important are:\n",
    "* ESC + M -- Change the current cell to use [**[M]**arkdown](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Working%20With%20Markdown%20Cells.html) instead of executing the code\n",
    "* ESC + Y -- Change  the current cell to P**[Y]**thon\n",
    "* ESC + L -- Show **[L]**ine numbers\n",
    "* ESC + A -- Add a new cell **[A]**bove the current cell\n",
    "* ESC + B -- Add a new cell **[B]**elow the current cell\n",
    "* ENTER -- Start editing the cell egain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the coolest features of jupyter is that you can display non-textual output such as plots or figures right in the notebook. \n",
    "\n",
    "Some of the most popular plotting and scientific options can be automatically imported into your notebook's python ineterprer using the `%pylab inline` directive to jupyter. \n",
    "\n",
    "> **NOTE:** Jupyter interprets lines that start with `%` as speciel ('magic') directives to the jupyter program rather than as python statements. If a cell starts with a double percent-sign (`%%`) then it can change the way that entire cell is interprete. For example, you can cause it to interpret cells as `C++` code or as `bash` statements to be executed in a subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a polynomial to fit some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This section was written during my office hours on Monday, let's see how far we can get in class _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to illustrate some of the concepts of chapter 1, I want to start by fitting some parametric models to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has a portion that explores the ideas presented in the textbook, which aims to give you a solid mathematical foundation in order to better understand machine learning approaches. \n",
    "\n",
    "I will also give some authentic examples of problems solved by regression.\n",
    "\n",
    "* Cleck [here](#Boston) to try out regression on a realistic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by fitting an order 1 polynomial (a line) to some scattered points. \n",
    "\n",
    "First, lets generate a random line.\n",
    "\n",
    "It will have the equation:\n",
    "$$ y(x) = w_0 x^0 + w_1 x^1 = w_0 + w_1 x$$\n",
    "\n",
    "Let us generate the $\\mathbf{w}$ vector by sampling from the 2-variable normal distribution $\\mathcal{N}(\\mathbf{w}|\\mathbf{0}, \\alpha^{-1}\\mathbf{I})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed(128)  #<-- make sure we all get the same 'random' results\n",
    "alpha = 1.0\n",
    "w = normal(scale=1./alpha, size=2)\n",
    "print w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some explanation:\n",
    "* The `seed` function initializes the state of the simple psueudo-random number generator. \n",
    "* The `alpha` variable is the precision if the Gaussian distribution that we assume our points come from. \n",
    "* The `normal` function is the (pythonic) way to generate numbers from a normal distribution. \n",
    "* There is also a convenient `randn(size)` function, but it always has variance of one. To set the precision to `alpha` I would need to divide by the square-root of alpha.\n",
    "* The weights, `w` are normally distributed, which means that the line is not likely to have a steep slope and that it is 50% likely to have a negative y-intercept. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate points from the line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = linspace(0,1,10)\n",
    "X = column_stack([x**0, x**1])\n",
    "y = X.dot(w)\n",
    "\n",
    "figsize(6, 4)\n",
    "plot(x, y, c='green', marker='*');\n",
    "axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of explanation:\n",
    "1. The `linspace` function is from the `numpy` package, which provides numerical operations on tensors. This function generates 10 equally spaced numbers betweeen 0 and 1 (including both 0 and 1). \n",
    "2. The `column_stack` function is from `numpy`, we are providing it with a list of vectors (1D tensors) and asking it to make a matrix (2D tensor) with those as its columns. \n",
    "3. In `numpy`, the `dot` method does matrix multiplication [using an `*` would multiply elementwise]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So,  there is a data matrix $\\mathbf{X}$ that has a row for each sample ($x_i$) and a column for each feature $[\\phi(x_n)]_k = x_n^k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to imagine that each point on our curve is actually subject to some error. For simplicity, we will assume that all error is in the _vertical_ direction; that is, we are super-precise about the $x$ that we measure but the $y$ may have some noise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = 10\n",
    "error = normal(scale=1./beta, size=len(y))\n",
    "t = y + error\n",
    "\n",
    "figsize(6, 4)\n",
    "plot(x, y, color='green', marker='*', zorder=2)\n",
    "scatter(x, t, facecolor='white', edgecolors='blue', zorder=3)\n",
    "vlines(x, y, t, color='red', zorder=1)\n",
    "\n",
    "axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have a data matrix $\\mathbf{X}$ and measurments $\\mathbf{t}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create that dataset, I used a known $\\mathbf{w}$ but in practice the line's parameters ar unknown; we only have $\\mathbf{X}$ and $\\mathbf{t}$ and the assumption that $\\mathbf{t} = \\mathbf{X}^T\\mathbf{t}+E$ where $E$ follows a normal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figsize(6,4)\n",
    "scatter(x, t, facecolor='white', edgecolors='blue', zorder=3)\n",
    "axis('equal')\n",
    "xlabel('$x$')\n",
    "ylabel('$t$')\n",
    "title(\"Input to an ML algorithm\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "In the next cell, write a function `phi(x, order)`.  The docstring for the function describes what it should do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phi(x, order):\n",
    "    \"\"\" Generate polynomial features for input `x`\n",
    "    \n",
    "    :param x: Either: (1) An array of $x$ values, or (2) as single $x$ value. \n",
    "    :param order: The order of the polynomial\n",
    "    \n",
    "    :return: A matrix `X` with `order+1` columns and `len(x)` \n",
    "             columns, so that `X[i,j]==x[i]**j`\n",
    "    \"\"\"\n",
    "    x = atleast_1d(x) # <-- Make sure the input is an array\n",
    "    M = order  # <-- Notation from the book\n",
    "    N = len(x) \n",
    "    \n",
    "    return np.zeros((N, M+1))  #FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert np.allclose(phi(0.5, order=2), [1., 0.5, 0.25])\n",
    "assert np.allclose(phi(x, order=1), X), \"Your output does not match the expected for our examples\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Write a function `eval_poly(x, w)` that takes as input a 1D array `x` and an array `w` of weights, and produces as output a 1D array `y`. \n",
    "> NOTE: It  is often useful to pass around $\\mathbf{\\phi}(\\mathbf{x})$ instead of $\\mathbf{x}$ for several reasons. Since python does not allow overloading functions, I have added some code to check whether or not a 1D or 2D array was passed in as the `x` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_poly(x, w):\n",
    "    x = atleast_1d(x)\n",
    "    M = len(w)-1\n",
    "    N = len(x)\n",
    "    if len(x.shape) == 1:\n",
    "        x = phi(x, order=M)\n",
    "        \n",
    "    # TODO: Try to get it in one line!\n",
    "    return zeros(N) #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Write a function `sum_squared_error(x, w)` that takes as input _either_ a 1D array _or_ a 2D array returned from a prior call to `phi`, along with target values `t` and weights `w`. It should return $$E(\\mathbf{w})=\\frac{1}{2}\\sum_{n=1}^{N}\\left\\{y(x_n, \\mathbf{w})-t_n\\right\\}^2,$$ from (1.2) of your book, however I want you to _avoid using loops_, instead try to use optimized `numpy` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_squared_error(x, w):\n",
    "    x = atleast_1d(x)\n",
    "    N = len(x)\n",
    "    M = len(w)-1\n",
    "    \n",
    "    # Allow the first parameter to be either the array of `x` values, _or_ the \n",
    "    # complete feature matrix. \n",
    "    if len(x.shape)  == 1:\n",
    "        x = phi(x, order=M)\n",
    "    \n",
    "    # TODO: Try to avoid loops!\n",
    "    return float('NaN') # FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"The sum_squared_error of our test data,\" +\n",
    "      \"using the true `w` that generated it, is\"), sum_squared_error(x, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Write a function that, given a parameter $\\mathbf{w}$, along with an input matrix $\\mathbf{X}=\\phi(\\mathbf{x})$ and the targets $\\mathbf{t}$, evaluates the negative log likelihood $- \\ln p( \\mathbf{t} |\\mathbf{x}, \\mathbf{w}, \\beta)$, where $\\beta$ is the precision of the noise (e.g. 10 in our case I beleive).\n",
    "> *NOTE:* I tend to pass around the _matrix_ $\\mathbf{X} = \\mathbf{\\phi}(\\mathbf{x})$ because\n",
    "> 1. I do not want to keep re-evaluating `phi` unnecessarily\n",
    "> 2. I feel like it reinforces the idea that what we are doing is _linear_, as in _linear algebra_. \n",
    "> \n",
    "> Since it the callers may not expect this, I will check whether the input is 1D or 2D and call $\\phi(\\mathbf{x})$ myself if necessary. \n",
    "\n",
    "Equation (1.62) of your book is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ln p(\\mathbf{t}|\\mathbf{x}, \\mathbf{w}, \\beta) &= -\\frac{\\beta}{2}\\sum_{n=1}^N \\left\\{y(x_n, \\mathbf{w})-t_n\\right\\}^2 + \\frac{N}{2}\\ln \\beta -\\frac{N}{2}\\ln(2\\pi).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "However, most optimization libaries want to _minimize_ some objective function, so it is useful to calculate the _negative_ log likelihood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neg_log_likelihood(t, x, w, beta=1):\n",
    "    \"\"\" Evaluate the negative log likelihood of the data \n",
    "    \n",
    "    Given data (x, t), and a setof linear weights `w` and noise precision `beta`, \n",
    "    find the negative natural log of the likelihood. \n",
    "    \n",
    "    See eq. (1.62) of your book, but bear in mind I want the _negative_ log likelihood.\n",
    "    \"\"\"\n",
    "    t = atleast_1d(t)\n",
    "    x = atleast_1d(x)\n",
    "    N = len(t)\n",
    "    M = len(w)-1\n",
    "    \n",
    "    # TODO\n",
    "        \n",
    "    return float('nan') #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your textbook talks alot about the joint probability $p(t, x| \\mathbf{w},\\beta)$ or the conditional probability $p(t|x, \\mathbf{w}, \\beta)$. I think it is useful to try and plot the conditional probabilities for all $t$ and $x$ now that we have functions to calculate the likelihood. \n",
    "\n",
    "Some explanation of the code below:\n",
    "* The (vertical) range of the data is unpredictable; it is the result of random $\\mathbf{w}$.  I want top plot a $1\\times1$ region, so I use the _center_ $y$ coordinate $c_y \\pm \\frac{1}{2}$ as the vertical range.\n",
    "* In line 3 have already used the variable names `x` and `y`, so I declare two new variable using trailing underscored. *Trailing underscores are how we resolve name clashes in python*.\n",
    "* Also in line 3, The `mgrid` object generates a 'mesh grid'; that is, it gives the $x$ and $y$ coordinates of every point on a grid. The number spacing of the grid elements is specified using python _slicing_ notation,\n",
    "> *IMPORTANT*: Slicing is critical to effective use of python, read [here](https://www.tutorialspoint.com/numpy/numpy_indexing_and_slicing.htm) or [here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html) or [here](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.mgrid.html) for more. \n",
    "* In general, loops are bad in python. Given some time, I could probably find a non-loopy way to do lines 6-8, but it was not clear to me how to do so and the number of iterations (10,000) is small. This is a case where I am comfortable 'breaking' that rule of thumb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the NLL (Negative Log Likelihoods), discussed above\n",
    "cy = (max(y)+min(y))/2.\n",
    "y_, x_ = mgrid[cy-0.5:cy+0.5:100j, 0:1:100j]\n",
    "nll = zeros((100,100))\n",
    "\n",
    "for j in range(100):\n",
    "    for i in range(100):\n",
    "        nll[i,j] = neg_log_likelihood(y_[i,j], x_[i,j], w, beta=10)\n",
    "\n",
    "p = exp(-nll)\n",
    "\n",
    "# Plot The Figure, discussed below\n",
    "figsize(10.5,8)\n",
    "imshow(p, extent=[0, 1, cy-0.5, cy+0.5], origin='lower', cmap=cm.gray);\n",
    "colorbar()\n",
    "contours = contour(x_, y_, p, cmap=cm.gray_r)\n",
    "clabel(contours)\n",
    "scatter(x, t)\n",
    "plot(x, y)\n",
    "xlabel('x')\n",
    "ylabel('y')\n",
    "title(r'The likelihood $p(y|x,\\mathbf{w},\\beta)$')\n",
    "axis('equal')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some explanation of the plotting code:\n",
    "* The probabilities are in a 2D array `p`, which can be displayed with the `imshow` command. \n",
    "* By default, images are drawn with p[0,0] on the top-left; however in our images p[0,0] should be onthe bottom left. Hence, I passed `origin = lower`. \n",
    "* By default, `imshow` assumes pixel-indices = $(y,x)$ coordinates; however our $100\\times100$ grid belongs in a different region. We use `extent=(xmin, xmax, ymin, ymax)` to position the image in the plot.  \n",
    "* The `countour` function draws _iso-lines_ where the surface takes on constant values, and the `clabel` function modifies the graphics-object returned by `countour` by adding labels to each isoline. \n",
    "* When plotting data, I always like to keep an _equal_ aspect ration (do not stretch the $y$'s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior probabilities\n",
    "Our weights $\\mathbf{w}$ where generated according to a Normal distibution with a mean of zero and a variance of one (scroll up, its true!). In practice, we may or may not know which distribution the wights come from, but heuristically we see good results if we assume polynomial weights are generated by zero-mean gaussians $$\\mathbf{w} \\sim \\mathcal{N}(\\mathbf{w}|\\mathbf{0}, \\alpha^{-1}\\mathbf{I})$$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the book (1.65), we can see that the prior $p(\\mathbf{w}|\\alpha)$ can be expressed as \n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{w}|\\alpha) &= \\mathcal{N}(\\mathbf{w}|\\mathbf{0}, \\alpha^{-1}\\mathbf{I}) \\\\\n",
    "                     &= \\left(\\frac{\\alpha}{2\\pi}\\right)^{(M+1)/2} \\exp\\left\\{-\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\right\\}\\\\ \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In practice, it is often better to work with the negative log likelihood (NLL), which we often seek to minimize. \n",
    "\n",
    "Can you come up with an expression for the negative log likelihood and type it out(using latex) in the next cell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Write an expression for $-\\ln p(\\mathbf{w}|\\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "-\\ln p(\\mathbf{w}|\\alpha) &= \\text{TODO} & \\text{(practice your \\LaTeX)}\\\\\n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an expression for the likelihood and that prior, we have \n",
    "$$\n",
    "\\underbrace{p(\\mathbf{w}|\\mathbf{X}, \\mathbf{t}, \\alpha, \\beta)}_{\\text{posterior}} \n",
    "\\propto \n",
    "\\underbrace{p(\\mathbf{t}|\\mathbf{X}, \\mathbf{w}, \\beta)}_{\\text{likelihood}} \n",
    "\\times \n",
    "\\underbrace{p(\\mathbf{w}|\\alpha)}_{\\text{prior}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the minimum\n",
    "$$\n",
    "\\begin{align}\n",
    "w &= \\arg\\min_{\\mathbf{w}} \\left\\{ -\\ln p(\\mathbf{t}| \\mathbf{X}, \\mathbf{w}, \\beta) \\right\\}\\\\\n",
    "  &= \\arg\\min_{\\mathbf{w}} \\left\\{  \\frac{\\beta}{2}||\\mathbf{X}\\mathbf{w}-\\mathbf{t}||^2-\\frac{N}{2}\\ln\\beta + \\frac{N}{2}\\ln(2\\pi) \\right\\}\\\\\n",
    "  &= \\arg\\min_{\\mathbf{w}}   \\left\\{\\frac{1}{2}||\\mathbf{X}\\mathbf{w}-\\mathbf{t}||^2 \\right\\} \\\\\n",
    "  &= \\arg\\min_{\\mathbf{w}}   \\left\\{\\frac{1}{2}(\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w}-2\\mathbf{w}^T\\mathbf{X}^T\\mathbf{t}+\\mathbf{t}^T\\mathbf{t}) \\right\\}\\\\\n",
    "    &= \\arg\\min_{\\mathbf{w}}   \\left\\{\\frac{1}{2}\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w}-\\mathbf{w}^T\\mathbf{X}^T\\mathbf{t}) \\right\\}\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "The derivative with respect to the vector $\\mathbf{w}$ is \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\mathbf{w}}\\left\\{ \\frac{1}{2}\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w}-\\mathbf{w}^T\\mathbf{X}^T\\mathbf{t}\\right\\} &= \\frac{1}{2}\\frac{\\partial}{\\partial \\mathbf{w}}\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} -\\frac{\\partial}{\\partial \\mathbf{w}}\\mathbf{t}^T\\mathbf{X}\\mathbf{w} \\\\\n",
    "&= \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} - \\mathbf{t}^T\\mathbf{X}\\\\\n",
    "\\left(\\frac{\\partial (\\cdot)}{\\partial \\mathbf{w}}\\right)^T&= \\mathbf{X}^T\\mathbf{X}\\mathbf{w} - \\mathbf{X}^T\\mathbf{t}\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "> The derivatives of matrix equations can be tricky; it is useful to consult a good linear algebra book or [this document](https://atmos.washington.edu/~dennis/MatrixCalculus.pdf).  Note that the rule we use is proposition 9 of that document; so that the the gradient $\\nabla f(\\mathbf{x}) = \\left(\\frac{\\partial f}{\\partial \\mathbf{x}}\\right)^T$ in our case. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Write a solution for the MLE for $\\mathbf{w}$ using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mle_w(X, t):\n",
    "    N = X.shape[0]\n",
    "    M = X.shape[1]-1\n",
    "    assert len(t) == N, \"The number of targets must match the number of inputs\"\n",
    "    \n",
    "    # Solving for 'w' is now just about a 1-liner....\n",
    "    return np.zeros(M+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Plot the curve along with a scatter plot of the points\n",
    "(Follow the example from the first part of this presentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted = X.dot(mle_w(X, t))\n",
    "#TODO: plot x vs y_predicted\n",
    "#TODO: scatter x, t\n",
    "#TODO: vertical error lines between y_predicted and t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)  Evaluate the sum-squared-error and compare to the error we get from the 'true' $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO:  MLE error is....\n",
    "#TODO:  Ground truth error is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Write a solution for the MAP estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_w(X, t, alpha, beta):\n",
    "    N = X.shape[0]\n",
    "    M = X.shape[1]-1\n",
    "    assert len(t) == N, \"The number of targets must match the number of inputs\"\n",
    "    \n",
    "    # Solving for 'w' is now just about a 1-liner....\n",
    "    return np.zeros(M+1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)  Plot your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am told that students have a hard time learning the concepts with a clear sense of their application. Linear regressions is covered in chapter one because it is so ubuquitious an, although it seems _easy_ to understand, there are many nuances to it that reveal the fundamental problems we encounter in machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use some more _realistic_ data, consider the problem of predicting the price of a home in Boston. This is a real dataset that has been used so much it has become a bit of a standard, and there is a simple python function to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print boston.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The plan\n",
    "1. I will show you the sklearn example that predicts home values given the other properties.\n",
    "2. We will dive in to figure out what these functions might be doing by re-implementing them ourselves. \n",
    "\n",
    "I did not have time to 'script' this part out, so it would be a work-along activity in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validation:\n",
    "predicted = sklearn.model_selection.cross_val_predict(lr, X, y, cv=10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y, predicted, edgecolors=(0, 0, 0))\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
