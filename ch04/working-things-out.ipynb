{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2.  probabilistic Generative Models\n",
    "## Derivation of the Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notation:\n",
    "$\\require{cancel}$\n",
    "$\\def\\x{\\mathbf{x}}$\n",
    "$\\def\\maha{\\Delta}$\n",
    "$\\def\\w{\\mathbf{w}}$\n",
    "$\\def\\X{\\mathbf{X}}$\n",
    "\n",
    "- let $\\w$ denote a vector of weights; $w_0$ is a bias parameter and $\\tilde\\w=(w_0,\\w)$ is the concatenated weights, where the ~ is omitted when clear. \n",
    "- Let $\\maha^2$ denote the squared mahanalobis distance (the part in the exponent of a Gaussian)\n",
    "- Let $\\x$ denote an arbitraty input sample, $\\x_n$ is the n'th sample in a set, $\\X$ is the entire training set. \n",
    "- $K$ is the number of classes, $D$ is the dimensionality of $\\x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know _Bayes_ theorem that \n",
    "$$ p(C_k | \\x) = \\frac{p(\\x | C_k) p (C_k)} {p(\\x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilitic Generative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the we are going to estimate the _likelihoods_ $p(x|C_k)$ and the priors $p(C_k)$, it is possible to estimate the probability of each sample $p(x)$. \n",
    "\n",
    "$$p(\\x) = \\sum_{j=1}^K P(\\x|C_j)p(C_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, since we know $p(\\x)$ it is possible to generate input data sets that are similar, so we call this as **generative** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative Model\n",
    "In principle, one can classify samples as long as you can partition the input region $x$ into regions $R_1, R_2, ... R_k$ so that $x\\in R_k$ means we decide to give $x$ the label $k$. Knowing the regions $R_k$ is not sifficient to generate new plausible samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilitic generative models are also discriminative, but they have more information about the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "k_\\text{pred} &= \\arg\\max_k p(C_k|x)\\\\\n",
    "p(C_k|x) &= \\frac{p(x | C_k) p (C_k)} {p(x)} & \\text{(Bayes)}\\\\\n",
    "  &=  \\frac{p(x | C_k) p (C_k)} {\\sum_j p(x | C_j) p (C_j)} & \\text{(Sum Rule)}\\\\\n",
    "  &= \\frac{\\exp\\{a_k\\}} {\\sum_j \\exp\\{a_j\\}}, & a_k = \\ln\\{p(x|C_k)p(C_k)\\}\n",
    "\\end{align}$$\n",
    "So the softmax is just rewriting Bayes therem, so that we can focus on the log-space value $a_k$. This is useful because probabilities of independant events are multiplied, which is numerically unstable. It is more convenient to use logarithm because products become sums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** Your book uses the variable $a_k$ for 'activation', which is a term related to neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriviing the sigmoid function for binary classification\n",
    "We have the softmax\n",
    "$$\\begin{align}\n",
    "p(C_k|x)  &= \\frac{\\exp\\{a_k\\}} {\\sum_j \\exp\\{a_j\\}}, & a_k = \\ln\\{p(x|C_k)p(C_k)\\}\\\\\n",
    "p(C_1|x) &=  \\frac{\\exp\\{a_1\\}} {\\exp\\{a_1\\}+\\exp\\{a_2\\}} & \\text{(For $K=2$))}\\\\\n",
    "&=  \\frac{1} {1+\\exp\\{a_2\\}/\\exp\\{a_1\\}} & \\text{reduce the fraction}\\\\\n",
    "&=  \\frac{1} {1+\\exp\\{a_2-a_1\\}} & \\text{combine exponents}\\\\\n",
    "&=  \\frac{1} {1+\\exp\\{-(a_1-a_2)\\}} & \\text{}\\\\\n",
    "&=  \\frac{1} {1+\\exp\\{-a\\}} & a=\\ln\\frac{p(x | C_1) p (C_1)}{p(x | C_2) p (C_2)}\\\\\n",
    "&= \\sigma(a)\n",
    "\\end{align}$$\n",
    "which is the sigmoid function, useful for binary classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (Multinomial)\n",
    "$\\def\\norm{\\mathcal{N}}$\n",
    "Let's make some **assumptions**:\n",
    " - Each class $C_k$ is normally distributed data; that is, $p(x|C_k) = \\norm(x|\\mu_k, \\Sigma)$.\n",
    " - All classes share the _**same**_ covariance, $\\Sigma$.\n",
    " \n",
    "Our **goal** here is to find definitions for $a_k$ in the definition of a sigmoid for multinomial classification.  In particular, we will represent each $a_k$ as a _linear_ function of $\\x$ so for _some_ choice of $\\w_k, w_{k0}$ we will find $a_k = \\w_k^T\\x+w_{k0}$.  It turns out that, given our assumptions above, things will work out nicely for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "p(x|C_1)  &=  \\frac{p(x | C_k) p (C_k)} {\\sum_j p(x | C_j) p (C_j)} & \\text{(see above)}\\\\\n",
    "&= \\frac{\\norm(x|\\mu_1,\\Sigma)p(C_1)}{\\sum_j \\norm(x|\\mu_j,\\Sigma)p(C_j)} &\\text{Gaussian asumption}\\\\\n",
    "&= \\frac{\n",
    "\\cancel{(2\\pi)^{-D/2}|\\Sigma|^\\frac{1}{2}}\\exp\\{\\maha^2_1)\\}p(C_1)}{\\sum_j \\cancel{(2\\pi)^{-D/2}|\\Sigma|^\\frac{1}{2}}\\exp\\{\\maha^2_j\\}p(C_j) } & \\text{where $\\Delta_j$ is mahanlobis dist.}\\\\\n",
    "&= \\frac{\n",
    "\\exp\\{-\\frac{1}{2}(\\x-\\mu_1)^T\\Sigma^{-1}(\\x-\\,u_1)\\}p(C_1)}{\\sum_j \\exp\\{\\maha^2_j\\}p(C_j) } & \\text{subst. $\\Delta^2_1$}\\\\\n",
    "&= \\frac{\n",
    "\\exp\\{-\\frac{1}{2}\\x^T\\Sigma^{-1}\\x + \\mu_1^T\\Sigma^{-1}\\x  -\\frac{1}{2}\\mu_1^T\\Sigma^{-1}\\mu_1\\}p(C_1)}{\\sum_j \\exp\\{\\maha^2_j\\}p(C_j) } & \\text{expand}\\\\\n",
    "&= \\frac{\n",
    "\\exp\\{-\\frac{1}{2}\\x^T\\Sigma^{-1}\\x\\}\\exp\\{\\mu_1^T\\Sigma^{-1}\\x  -\\frac{1}{2}\\mu_1^T\\Sigma^{-1}\\mu_1\\}p(C_1)}{\\sum_j \\exp\\{\\maha^2_j\\}p(C_j) } & \\text{Split the exponent}\\\\\n",
    "&= \\frac{\n",
    "\\cancel{\\exp\\{-\\frac{1}{2}\\x^T\\Sigma^{-1}\\x\\}}\\exp\\{\\mu_1^T\\Sigma^{-1}\\x  -\\frac{1}{2}\\mu_1^T\\Sigma^{-1}\\mu_1\\}p(C_1)}\n",
    "{\\sum_j \\cancel{\\exp\\{-\\frac{1}{2}\\x^T\\Sigma^{-1}\\x\\}}\\exp\\{\\mu_j^T\\Sigma^{-1}\\x  -\\frac{1}{2}\\mu_j^T\\Sigma^{-1}\\mu_j\\}p(C_j) } & \\text{ditto for the denom.}\\\\\n",
    "&= \\frac{\n",
    "\\exp\\{\\mu_1^T\\Sigma^{-1}\\x  -\\frac{1}{2}\\mu_1^T\\Sigma^{-1}\\mu_1\\}p(C_1)}\n",
    "{\\sum_j \\exp\\{\\mu_j^T\\Sigma^{-1}\\x  -\\frac{1}{2}\\mu_j^T\\Sigma^{-1}\\mu_j\\}p(C_j) } & \\text{}\\\\\n",
    "&= \\frac{\n",
    "\\exp\\{\\mu_1^T\\Sigma^{-1}\\x  -\\frac{1}{2}\\mu_1^T\\Sigma^{-1}\\mu_1 + \\ln p(C_1)\\}}\n",
    "{\\sum_j \\exp\\{\\mu_j^T\\Sigma^{-1}\\x  -\\frac{1}{2}\\mu_j^T\\Sigma^{-1}\\mu_j +\\ln p(C_j)\\} } & \\text{combine exponents ($\\exp(\\ln ..)$)}\\\\\n",
    "&= \\frac{\n",
    "\\exp\\{(\\Sigma^{-1}\\mu_1)^T\\x  -\\frac{1}{2}\\mu_1^T\\Sigma^{-1}\\mu_1 + \\ln p(C_1)\\}}\n",
    "{\\sum_j \\exp\\{\\underbrace{(\\Sigma^{-1}\\mu_j)^T}_{\\w_j^T}\\x  \\underbrace{-\\frac{1}{2}\\mu_j^T\\Sigma^{-1}\\mu_j +\\ln p(C_j)}_{w_{j0}}\\} } & \\text{play with transpose}\\\\\n",
    "&= \\frac{\n",
    "\\exp\\{\\w_1^T\\x + w_{10}\\}}\n",
    "{\\sum_j \\exp\\{\\w_j^T\\x + w_{j0}\\} } & \\text{subst. $\\w_j, w_{j0}$}\\\\\n",
    "&= \\frac{\n",
    "\\exp\\{a_1\\}}\n",
    "{\\sum_j \\exp\\{a_j\\} } & \\text{subst. $a_j=\\w_j^T\\x + w_{j0}$}\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden in there is my definition for $\\w_j$ and $w_{j0}$, \n",
    "$$\\w_j=\\Sigma^{-1}\\mu_j,  \\hspace{4em}w_{j0} = -\\frac{1}{2}\\mu_j^T\\Sigma^{-1}\\mu_j + \\ln p(C_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we choose the minimim error (max posterior) classification, the regions $\\{R_k\\}$ that result from setting $k_\\text{pred}=\\arg\\max_k p(C_k|x)$ are identical to the regions you would find by setting $k_\\text{pred}=\\arg\\max_k a_k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (Binary)\n",
    "This is actually the _most common_ version of logistic regression. \n",
    "\n",
    "Our assumptions are the same as above, with the addition of a binary class assumption:\n",
    " - Each class $C_k$ is normally distributes data; that is, $p(x|C_k) = \\norm(x|\\mu_k, \\Sigma)$.\n",
    " - All classes share the _**same**_ covariance, $\\Sigma$.\n",
    " - The number of classes ($K=2$), i.e. we are doing binary classification. \n",
    " \n",
    "\n",
    "For binary problems we have found\n",
    "\n",
    "$$\\begin{align}\n",
    "p(C_1|x) &= \\sigma(a) \\\\\n",
    "         &= \\frac{1}{1+\\exp\\{-a\\}}\n",
    "\\end{align}$$\n",
    "where \n",
    "$$\\begin{align}\n",
    "a &= a_1-a_2 \\\\\n",
    "  &= \\w_1^T\\x - \\w_2^T\\x + w_{10} - w_{20} \\\\\n",
    "  &= \\underbrace{(\\w_1 - \\w_2)}_{\\w}^T\\x + \\underbrace{(w_{10} - w_{20})}_{w_0}\\\\\n",
    "  &= \\w^T\\x+w_0\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "so \n",
    "$$\\begin{align}\n",
    "\\w &= \\w_1-\\w_2 \\\\\n",
    "  &= \\Sigma^{-1}\\mu_1 - \\Sigma^{-1}\\mu_2 \\\\\n",
    "  &= \\Sigma^{-1}(\\mu_1 - \\mu_2)\\\\\n",
    "w_0 &= w_{10} - w_{20} \\\\\n",
    "    &= \\underbrace{ -\\frac{1}{2}\\mu_1^T\\Sigma^{-1}\\mu_1 + \\ln p(C_1)}_{w_{10}} \n",
    "       \\underbrace{ +\\frac{1}{2}\\mu_2^T\\Sigma^{-1}\\mu_2 - \\ln p(C_2)}_{-w_{20}}\\\\\n",
    "    &= \\frac{1}{2}(\\mu_2^T\\Sigma^{-1}\\mu_2-\\mu_1^T\\Sigma^{-1}\\mu_1) + \\ln p(C_1)-\\ln p(C_2)\\\\\n",
    "    &= \\frac{1}{2}(\\mu_2-\\mu_1)^T\\Sigma^{-1}(\\mu_2-\\mu_1) + \\ln p(C_1)-\\ln p(C_2)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
